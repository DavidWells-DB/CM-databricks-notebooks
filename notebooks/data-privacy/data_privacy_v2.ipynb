{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Data Privacy and Security in Databricks\n",
    "\n",
    "This notebook demonstrates data privacy and security features in Databricks Unity Catalog.\n",
    "\n",
    "## Topics:\n",
    "1. **RBAC** - Role-Based Access Control\n",
    "2. **Views** - Dynamic, Restricted, and Materialized\n",
    "3. **Data Hashing** - Irreversible anonymization\n",
    "4. **Data Masking** - Format-preserving obfuscation\n",
    "5. **Row Filtering** - Scope access by attributes\n",
    "6. **Tokenization** - Reversible token replacement\n",
    "7. **ABAC** - Attribute-Based Access Control\n",
    "8. **Encryption** - Protect data at rest and in transit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your demo preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Configuration\n",
    "USE_TEMP_TABLES = True  # Recommended for demos - auto-cleanup on session end\n",
    "\n",
    "# Catalog and schema names\n",
    "CATALOG = \"main\"\n",
    "HR_SCHEMA = \"hr\"\n",
    "CUSTOMERS_SCHEMA = \"customers\"\n",
    "RETAIL_SCHEMA = \"retail\"\n",
    "GOVERNANCE_SCHEMA = \"governance\"\n",
    "\n",
    "print(f\"‚úì Configuration loaded\")\n",
    "print(f\"  ‚Üí Temporary tables: {USE_TEMP_TABLES}\")\n",
    "print(f\"  ‚Üí Catalog: {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to set up the demo environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup notebook (creates tables and sample data in background)\n",
    "%run ./setup_environment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and define helper function\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def get_table_type():\n",
    "    \"\"\"Helper to add TEMPORARY keyword when using temp tables\"\"\"\n",
    "    return \"TEMPORARY\" if USE_TEMP_TABLES else \"\"\n",
    "\n",
    "print(\"‚úì Libraries loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "masking_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Data Masking\n",
    "\n",
    "**What is Data Masking?**\n",
    "Replaces sensitive values with obfuscated versions while maintaining format and structure.\n",
    "\n",
    "**Use Cases:**\n",
    "- Display masked SSNs (XXX-XX-6789) instead of raw values\n",
    "- Preserve formats for analytics while hiding true values\n",
    "- Automate masking by user/group with Unity Catalog\n",
    "\n",
    "**Key Functions:** `IS_ACCOUNT_GROUP_MEMBER()`, `IS_MEMBER()`, `mask()`\n",
    "\n",
    "**Important:** Fine-grained controls require serverless compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "masking_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masking function based on group membership\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {CATALOG}.{GOVERNANCE_SCHEMA}.mask_ssn(ssn STRING)\n",
    "RETURNS STRING\n",
    "RETURN CASE\n",
    "    WHEN IS_ACCOUNT_GROUP_MEMBER('admin') THEN ssn\n",
    "    ELSE '***-**-****'\n",
    "END\"\"\")\n",
    "\n",
    "# Create view with masked SSN\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {CATALOG}.{RETAIL_SCHEMA}.v_customers_masked AS\n",
    "SELECT id, {CATALOG}.{GOVERNANCE_SCHEMA}.mask_ssn(ssn) AS ssn, name, region\n",
    "FROM {CATALOG}.{RETAIL_SCHEMA}.customers\"\"\")\n",
    "\n",
    "print(\"Original Data:\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{RETAIL_SCHEMA}.customers LIMIT 3\"))\n",
    "\n",
    "print(\"\\nMasked Data (SSN hidden based on permissions):\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{RETAIL_SCHEMA}.v_customers_masked LIMIT 3\"))\n",
    "\n",
    "print(\"\\n‚úì SSN masked based on group membership\")\n",
    "print(\"‚úì Admins see full SSN, others see masked\")\n",
    "print(\"‚úì Format preserved for analytics\")\n",
    "print(\"\\n‚ö†Ô∏è  Update 'admin' to your admin group name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filtering_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Row-Level Filtering\n",
    "\n",
    "**What is Row Filtering?**\n",
    "Controls which records users can view by applying row-level conditions, enforced transparently at query time.\n",
    "\n",
    "**Use Cases:**\n",
    "- GDPR: Restrict EU data to EU employees only\n",
    "- Multi-tenancy: Each customer sees only their data\n",
    "- Financial segmentation: Business units see only their accounts\n",
    "- Data sharing: Curated datasets for external partners\n",
    "\n",
    "**Benefits:** Transparent enforcement ‚Ä¢ Combines with column masks ‚Ä¢ No data duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filtering_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create row filter function based on region\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {CATALOG}.{GOVERNANCE_SCHEMA}.filter_by_region(region STRING)\n",
    "RETURNS BOOLEAN\n",
    "RETURN CASE\n",
    "    WHEN IS_MEMBER('Team_US') AND region = 'US' THEN TRUE\n",
    "    WHEN IS_MEMBER('Team_EU') AND region = 'EU' THEN TRUE\n",
    "    WHEN IS_MEMBER('Team_APAC') AND region = 'APAC' THEN TRUE\n",
    "    WHEN IS_ACCOUNT_GROUP_MEMBER('admin') THEN TRUE\n",
    "    ELSE FALSE\n",
    "END\"\"\")\n",
    "\n",
    "# Create view with row filtering\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {CATALOG}.{RETAIL_SCHEMA}.v_customers_filtered AS\n",
    "SELECT id, ssn, name, region\n",
    "FROM {CATALOG}.{RETAIL_SCHEMA}.customers\n",
    "WHERE {CATALOG}.{GOVERNANCE_SCHEMA}.filter_by_region(region)\"\"\")\n",
    "\n",
    "print(\"All Data (5 rows):\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{RETAIL_SCHEMA}.customers ORDER BY id\"))\n",
    "\n",
    "print(\"\\nFiltered Data (based on user's region):\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{RETAIL_SCHEMA}.v_customers_filtered ORDER BY id\"))\n",
    "\n",
    "print(\"\\n‚úì Team_US: US records only\")\n",
    "print(\"‚úì Team_EU: EU records only\")\n",
    "print(\"‚úì Team_APAC: APAC records only\")\n",
    "print(\"‚úì Admins: All records\")\n",
    "print(\"\\n‚ö†Ô∏è  Update Team_US, Team_EU, Team_APAC to your group names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Data Tokenization\n",
    "\n",
    "**What is Tokenization?**\n",
    "Substitutes sensitive values with random tokens that map back via secure vaults. Unlike hashing, tokenization is **reversible**.\n",
    "\n",
    "**Use Cases:**\n",
    "- PCI-DSS: Replace credit cards with compliant tokens\n",
    "- Testing: Provide realistic but protected test data\n",
    "- Analytics: Enable analysis without exposing PII\n",
    "- Fraud detection: Reversible for authorized investigation\n",
    "\n",
    "**Production Integration:** VGS, Basis Theory, TokenEx\n",
    "\n",
    "**Trade-offs:** ‚úì Reversible ‚Ä¢ ‚ö†Ô∏è Requires external service ‚Ä¢ ‚ö†Ô∏è Performance overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenization_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Simplified demo - production uses external vault services\n",
    "\n",
    "# Create tokenization functions\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {CATALOG}.{GOVERNANCE_SCHEMA}.tokenize(value STRING)\n",
    "RETURNS STRING\n",
    "RETURN CONCAT('TOK-', substr(sha2(value, 256), 1, 32))\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {CATALOG}.{GOVERNANCE_SCHEMA}.detokenize(token STRING, original STRING)\n",
    "RETURNS STRING\n",
    "RETURN CASE\n",
    "    WHEN IS_ACCOUNT_GROUP_MEMBER('admin') THEN original\n",
    "    ELSE token\n",
    "END\"\"\")\n",
    "\n",
    "# Create table with tokenized values\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} TABLE {CATALOG}.{RETAIL_SCHEMA}.customers_tokenized AS\n",
    "SELECT id, {CATALOG}.{GOVERNANCE_SCHEMA}.tokenize(ssn) AS ssn_token, ssn AS ssn_original, name, region\n",
    "FROM {CATALOG}.{RETAIL_SCHEMA}.customers\"\"\")\n",
    "\n",
    "# Create view with conditional detokenization\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {CATALOG}.{RETAIL_SCHEMA}.v_customers_tokenized AS\n",
    "SELECT id, {CATALOG}.{GOVERNANCE_SCHEMA}.detokenize(ssn_token, ssn_original) AS ssn, name, region\n",
    "FROM {CATALOG}.{RETAIL_SCHEMA}.customers_tokenized\"\"\")\n",
    "\n",
    "print(\"Tokenized Storage:\")\n",
    "display(spark.sql(f\"SELECT id, ssn_token, name, region FROM {CATALOG}.{RETAIL_SCHEMA}.customers_tokenized LIMIT 3\"))\n",
    "\n",
    "print(\"\\nConditional Detokenization:\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{RETAIL_SCHEMA}.v_customers_tokenized LIMIT 3\"))\n",
    "\n",
    "print(\"\\n‚úì Non-admins see tokens only\")\n",
    "print(\"‚úì Admins see original values\")\n",
    "print(\"\\nüîó Production: VGS ‚Ä¢ Basis Theory ‚Ä¢ TokenEx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Attribute-Based Access Control (ABAC)\n",
    "\n",
    "**What is ABAC?**\n",
    "Policy-driven access control based on object attributes (tags). Permissions set and enforced dynamically as data evolves.\n",
    "\n",
    "**Use Cases:**\n",
    "- Auto-deny access to columns tagged 'sensitivity=PII'\n",
    "- Monitor and protect credit card data automatically\n",
    "- Apply policies to new tables/columns with matching tags\n",
    "\n",
    "**Key Features:** Tag-based policies ‚Ä¢ Dynamic enforcement ‚Ä¢ Centralized governance\n",
    "\n",
    "**Status:** Currently in **Beta** (October 2025)\n",
    "\n",
    "[ABAC Documentation](https://docs.databricks.com/security/attribute-based-access-control.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABAC Conceptual Workflow (requires Beta workspace configuration)\n",
    "\n",
    "print(\"ABAC Workflow (Conceptual)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Step 1: Tag column as PII\")\n",
    "print(\"  ALTER TABLE hr.employee_info\")\n",
    "print(\"  ALTER COLUMN ssn SET TAGS ('sensitivity' = 'PII');\\n\")\n",
    "\n",
    "print(\"Step 2: Create policy to mask PII\")\n",
    "print(\"  CREATE POLICY mask_pii ON SCHEMA hr\")\n",
    "print(\"  COLUMN MASK (ssn) USING '***-**-****'\")\n",
    "print(\"  TO all_accounts EXCEPT hr_admins;\\n\")\n",
    "\n",
    "print(\"Step 3: Apply policy\")\n",
    "print(\"  ALTER TABLE hr.employee_info\")\n",
    "print(\"  ALTER COLUMN ssn SET MASK POLICY mask_pii;\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate with view\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {CATALOG}.{HR_SCHEMA}.v_employee_info_abac AS\n",
    "SELECT id, name, salary,\n",
    "    CASE WHEN IS_ACCOUNT_GROUP_MEMBER('hr_admin') THEN ssn ELSE '***-**-****' END AS ssn\n",
    "FROM {CATALOG}.{HR_SCHEMA}.employee_info\"\"\")\n",
    "\n",
    "print(\"Simulated ABAC Behavior:\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{HR_SCHEMA}.v_employee_info_abac\"))\n",
    "\n",
    "print(\"\\n‚úì Policy auto-masks columns tagged as PII\")\n",
    "print(\"‚úì Applies to all tables in schema\")\n",
    "print(\"‚úì Exceptions for specific groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encryption_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Data Encryption\n",
    "\n",
    "**What is Encryption?**\n",
    "Protects data at rest and in transit by converting to encoded format readable only with decryption keys.\n",
    "\n",
    "**Databricks Encryption Options:**\n",
    "\n",
    "1. **AES Functions** - Column-level encryption (`AES_ENCRYPT`, `AES_DECRYPT`)\n",
    "2. **Server-side** - Automatic cloud storage encryption (S3, Azure Blob, GCS)\n",
    "3. **Format-Preserving** - Encrypt while maintaining format\n",
    "4. **Envelope Encryption** - Multi-layer DEK/KEK approach\n",
    "5. **Multi-key Protection** - Customer + Databricks managed keys\n",
    "\n",
    "**See `data_encryption.ipynb` for detailed encryption demonstrations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encryption_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AES-128 Encryption Demo\n",
    "encryption_key = \"MySecureKey12345\"  # ‚ö†Ô∏è Use Azure Key Vault/AWS KMS/GCP KMS in production\n",
    "\n",
    "# Create table with encrypted SSN\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} TABLE {CATALOG}.{HR_SCHEMA}.employee_info_encrypted AS\n",
    "SELECT id, name, salary, base64(aes_encrypt(ssn, '{encryption_key}', 'ECB', 'PKCS')) AS ssn_encrypted\n",
    "FROM {CATALOG}.{HR_SCHEMA}.employee_info\"\"\")\n",
    "\n",
    "print(\"Encrypted Data:\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{HR_SCHEMA}.employee_info_encrypted\"))\n",
    "\n",
    "# Create view with conditional decryption\n",
    "spark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {CATALOG}.{HR_SCHEMA}.v_employee_info_decrypted AS\n",
    "SELECT id, name, salary,\n",
    "    CASE\n",
    "        WHEN IS_ACCOUNT_GROUP_MEMBER('hr_admin')\n",
    "        THEN aes_decrypt(unbase64(ssn_encrypted), '{encryption_key}', 'ECB', 'PKCS')\n",
    "        ELSE '***-**-****'\n",
    "    END AS ssn\n",
    "FROM {CATALOG}.{HR_SCHEMA}.employee_info_encrypted\"\"\")\n",
    "\n",
    "print(\"\\nConditionally Decrypted:\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{HR_SCHEMA}.v_employee_info_decrypted\"))\n",
    "\n",
    "print(\"\\n‚úì HR admins see decrypted values\")\n",
    "print(\"‚úì Others see masked values\")\n",
    "print(\"\\nüîí Best Practices:\")\n",
    "print(\"   ‚Ä¢ Use customer-managed keys (CMK)\")\n",
    "print(\"   ‚Ä¢ Rotate keys regularly\")\n",
    "print(\"   ‚Ä¢ Store keys in vault services (Key Vault, KMS)\")\n",
    "print(\"   ‚Ä¢ Enable TLS/SSL for data in transit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Privacy Features Comparison\n",
    "\n",
    "| Feature | Use Case | Reversible | Performance | Complexity |\n",
    "|---------|----------|------------|-------------|------------|\n",
    "| **RBAC** | Role-based permissions | N/A | Low | Low |\n",
    "| **Views** | Controlled exposure | N/A | Low-Med | Low |\n",
    "| **Hashing** | Anonymization | No | Low | Low |\n",
    "| **Masking** | Format-preserving obfuscation | Optional | Low-Med | Medium |\n",
    "| **Row Filtering** | Regional/attribute access | N/A | Medium | Medium |\n",
    "| **Tokenization** | Reversible PII protection | Yes | Med-High | High |\n",
    "| **ABAC** | Policy-driven control | N/A | Medium | Med-High |\n",
    "| **Encryption** | At-rest/transit protection | Yes | Low-Med | Medium |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "‚úì **Defense in Depth** - Combine techniques for comprehensive protection\n",
    "‚úì **Unity Catalog** - Centralized governance for all privacy controls\n",
    "‚úì **Serverless Compute** - Required for fine-grained controls\n",
    "‚úì **Audit & Compliance** - All controls logged and auditable\n",
    "‚úì **Performance** - Consider impact when implementing complex policies\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "- [Row & Column Filters](https://docs.databricks.com/security/privacy/row-and-column-filters.html)\n",
    "- [ABAC](https://docs.databricks.com/security/attribute-based-access-control.html)\n",
    "- [Encryption](https://docs.databricks.com/security/encryption/index.html)\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Configuration Notes\n",
    "\n",
    "**Update these values for your Databricks environment:**\n",
    "\n",
    "**Group Names:**\n",
    "- `admin` ‚Üí Your admin group\n",
    "- `hr_admin` / `hr_viewer_group` ‚Üí Your HR groups\n",
    "- `Team_US` / `Team_EU` / `Team_APAC` ‚Üí Your regional groups\n",
    "\n",
    "**Encryption:**\n",
    "- Replace hardcoded keys with Azure Key Vault / AWS KMS / GCP KMS references\n",
    "\n",
    "**Catalogs/Schemas:**\n",
    "- Adjust in Configuration cell if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup (Only for Permanent Tables)\n",
    "\n",
    "If you used permanent tables (`USE_TEMP_TABLES = False`), run the cleanup cell below.\n",
    "\n",
    "**Note:** Temporary tables are automatically cleaned up when your session ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (only runs if using permanent tables)\n",
    "if not USE_TEMP_TABLES:\n",
    "    print(\"Cleaning up permanent tables and views...\")\n",
    "    \n",
    "    # Drop views\n",
    "    for view in ['employee_info_public', 'v_employee_info_abac', 'v_employee_info_decrypted']:\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{HR_SCHEMA}.{view}\")\n",
    "    \n",
    "    for view in ['v_customers_private']:\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{CUSTOMERS_SCHEMA}.{view}\")\n",
    "    \n",
    "    for view in ['v_customers_masked', 'v_customers_filtered', 'v_customers_tokenized']:\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{RETAIL_SCHEMA}.{view}\")\n",
    "    \n",
    "    # Drop tables\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{HR_SCHEMA}.employee_info\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{HR_SCHEMA}.employee_info_encrypted\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{CUSTOMERS_SCHEMA}.customer_info\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{RETAIL_SCHEMA}.customers\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{RETAIL_SCHEMA}.customers_tokenized\")\n",
    "    \n",
    "    # Drop functions\n",
    "    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.mask_ssn\")\n",
    "    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.filter_by_region\")\n",
    "    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.tokenize\")\n",
    "    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.detokenize\")\n",
    "    \n",
    "    print(\"‚úì Cleanup complete\")\n",
    "else:\n",
    "    print(\"Using temporary tables - no cleanup needed!\")\n",
    "    print(\"Tables will be automatically removed when session ends.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
