{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37653745-fd3f-448e-883d-6b106e53cb09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encryption in Databricks\n",
    "\n",
    "This notebook provides in-depth demonstrations of data encryption techniques available in Databricks.\n",
    "\n",
    "## Topics Covered:\n",
    "\n",
    "1. **Server-side Encryption for Cloud Storage Services** - Automatic encryption at rest\n",
    "2. **AES Encryption/Decryption** - Column-level encryption using AES algorithms\n",
    "3. **Format-Preserving Encryption (FPE)** - Maintain data format while encrypting\n",
    "4. **Envelope Encryption with Unity Catalog** - Multi-layer encryption approach\n",
    "5. **Databricks Multi-key Protection** - Customer-managed + Databricks-managed keys\n",
    "\n",
    "---\n",
    "\n",
    "### Why Encryption?\n",
    "\n",
    "Encryption is a critical component of data security that:\n",
    "- **Protects data at rest** in cloud storage\n",
    "- **Protects data in transit** between systems\n",
    "- **Ensures compliance** with regulations (GDPR, HIPAA, PCI-DSS)\n",
    "- **Prevents unauthorized access** even if storage is compromised\n",
    "- **Provides customer control** with customer-managed keys (CMK)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries and configure environment\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Configuration\n",
    "CATALOG = \"main\"\n",
    "ENCRYPTION_SCHEMA = \"encryption_demo\"\n",
    "\n",
    "# Create schema for encryption demonstrations\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{ENCRYPTION_SCHEMA}\")\n",
    "\n",
    "print(\"‚úì Environment setup complete\")\n",
    "print(f\"‚Üí Using catalog: {CATALOG}\")\n",
    "print(f\"‚Üí Using schema: {ENCRYPTION_SCHEMA}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Server-side Encryption for Cloud Storage\n",
    "\n",
    "**What is Server-side Encryption?**\n",
    "\n",
    "Server-side encryption (SSE) automatically encrypts data when it's written to cloud storage and decrypts it when accessed. This is managed by the cloud provider and requires no application changes.\n",
    "\n",
    "**Cloud Provider Options:**\n",
    "\n",
    "### AWS S3\n",
    "- **SSE-S3:** Amazon S3-managed keys (AES-256)\n",
    "- **SSE-KMS:** AWS Key Management Service (customer-managed keys)\n",
    "- **SSE-C:** Customer-provided encryption keys\n",
    "\n",
    "### Azure Blob Storage\n",
    "- **Microsoft-managed keys:** Automatic encryption with Azure-managed keys\n",
    "- **Customer-managed keys:** Use Azure Key Vault for key management\n",
    "\n",
    "### Google Cloud Storage\n",
    "- **Google-managed keys:** Default encryption at rest\n",
    "- **Customer-managed encryption keys (CMEK):** Use Cloud KMS\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úì Transparent to applications\n",
    "- ‚úì No performance overhead\n",
    "- ‚úì Enabled by default in most cloud providers\n",
    "- ‚úì Compliant with security standards\n",
    "\n",
    "**Databricks Integration:**\n",
    "- All data stored in Delta Lake is encrypted at rest by default\n",
    "- Additional encryption layers can be configured via workspace settings\n",
    "- Customer-managed keys provide additional control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server-side Encryption is automatic in Databricks\n",
    "# This cell demonstrates that your data is already encrypted at rest\n",
    "\n",
    "print(\"Server-side Encryption Status\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a sample table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{ENCRYPTION_SCHEMA}.sensitive_data (\n",
    "    id INT,\n",
    "    credit_card STRING,\n",
    "    customer_name STRING,\n",
    "    transaction_amount DECIMAL(10, 2)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample data\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{ENCRYPTION_SCHEMA}.sensitive_data VALUES\n",
    "    (1, '4532-1111-2222-3333', 'John Doe', 1250.00),\n",
    "    (2, '5555-4444-3333-2222', 'Jane Smith', 3400.50),\n",
    "    (3, '3782-123456-78901', 'Bob Johnson', 890.25)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úì Table created with sensitive data\")\n",
    "print(\"‚Üí Data is automatically encrypted at rest by your cloud provider\")\n",
    "print(\"‚Üí Encryption happens transparently without application changes\")\n",
    "print(\"‚Üí Data is decrypted automatically when read by authorized users\\n\")\n",
    "\n",
    "# Show the table (data appears unencrypted because we have access)\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{ENCRYPTION_SCHEMA}.sensitive_data\"))\n",
    "\n",
    "print(\"\\nüìù Note: While you see unencrypted data (because you're authorized),\")\n",
    "print(\"   the actual storage files are encrypted using AES-256 encryption.\")\n",
    "print(\"   Without proper credentials, the underlying files are unreadable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. AES Encryption and Decryption\n",
    "\n",
    "**What is AES Encryption?**\n",
    "\n",
    "AES (Advanced Encryption Standard) is a symmetric encryption algorithm that provides column-level encryption in addition to storage-level encryption. This allows you to encrypt specific sensitive fields within your data.\n",
    "\n",
    "**AES Key Sizes:**\n",
    "- **AES-128:** 16-byte key (sufficient for most use cases)\n",
    "- **AES-192:** 24-byte key (higher security)\n",
    "- **AES-256:** 32-byte key (maximum security)\n",
    "\n",
    "**Modes of Operation:**\n",
    "- **ECB (Electronic Codebook):** Simplest mode, deterministic\n",
    "- **CBC (Cipher Block Chaining):** More secure, uses initialization vector\n",
    "- **GCM (Galois/Counter Mode):** Authenticated encryption\n",
    "\n",
    "**Padding Schemes:**\n",
    "- **PKCS:** Standard padding for block ciphers\n",
    "- **NONE:** No padding (data must be block-aligned)\n",
    "\n",
    "**SQL Functions:**\n",
    "- `AES_ENCRYPT(input, key, mode, padding)` - Encrypts data\n",
    "- `AES_DECRYPT(input, key, mode, padding)` - Decrypts data\n",
    "- `BASE64(binary)` - Encodes binary to Base64 string\n",
    "- `UNBASE64(string)` - Decodes Base64 string to binary\n",
    "\n",
    "**Use Cases:**\n",
    "- Encrypt credit card numbers, SSNs, and other PII\n",
    "- Store encrypted data while allowing queries on other columns\n",
    "- Implement field-level encryption for compliance\n",
    "- Control who can decrypt data via key management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AES-128 Encryption Example\n",
    "\n",
    "print(\"AES-128 Encryption Demonstration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define encryption key (16 bytes for AES-128)\n",
    "# ‚ö†Ô∏è In production, NEVER hardcode keys - use a secure key management service\n",
    "encryption_key_128 = \"SecureKey1234567\"  # 16 characters = 16 bytes\n",
    "\n",
    "print(f\"\\n‚Üí Encryption Key Length: {len(encryption_key_128)} bytes (AES-128)\")\n",
    "print(\"‚Üí Algorithm: AES\")\n",
    "print(\"‚Üí Mode: ECB\")\n",
    "print(\"‚Üí Padding: PKCS\\n\")\n",
    "\n",
    "# Create a table with encrypted credit card numbers\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{ENCRYPTION_SCHEMA}.payments_encrypted AS\n",
    "SELECT \n",
    "    id,\n",
    "    customer_name,\n",
    "    base64(aes_encrypt(credit_card, '{encryption_key_128}', 'ECB', 'PKCS')) AS credit_card_encrypted,\n",
    "    transaction_amount\n",
    "FROM \n",
    "    {CATALOG}.{ENCRYPTION_SCHEMA}.sensitive_data\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Table created with encrypted credit card numbers\\n\")\n",
    "\n",
    "# Display the encrypted data\n",
    "print(\"Encrypted Data (credit card numbers are encrypted):\")\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{ENCRYPTION_SCHEMA}.payments_encrypted\"))\n",
    "\n",
    "print(\"\\nüìù Note: Credit card numbers are now stored in encrypted form.\")\n",
    "print(\"   Only users with the encryption key can decrypt them.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AES Decryption Example\n",
    "\n",
    "print(\"AES-128 Decryption Demonstration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a view that decrypts the credit card numbers\n",
    "print(\"\\nCreating a view that decrypts credit card numbers for authorized users...\\n\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {CATALOG}.{ENCRYPTION_SCHEMA}.v_payments_decrypted AS\n",
    "SELECT \n",
    "    id,\n",
    "    customer_name,\n",
    "    CASE \n",
    "        WHEN IS_ACCOUNT_GROUP_MEMBER('payment_processors') THEN \n",
    "            aes_decrypt(unbase64(credit_card_encrypted), '{encryption_key_128}', 'ECB', 'PKCS')\n",
    "        ELSE '****-****-****-****'\n",
    "    END AS credit_card,\n",
    "    transaction_amount\n",
    "FROM \n",
    "    {CATALOG}.{ENCRYPTION_SCHEMA}.payments_encrypted\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì View created with conditional decryption\\n\")\n",
    "\n",
    "# Display the decrypted data (for authorized users)\n",
    "print(\"Decrypted Data (for demonstration - in production, only authorized users would see this):\")\n",
    "decrypted_df = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    customer_name,\n",
    "    aes_decrypt(unbase64(credit_card_encrypted), '{encryption_key_128}', 'ECB', 'PKCS') AS credit_card,\n",
    "    transaction_amount\n",
    "FROM \n",
    "    {CATALOG}.{ENCRYPTION_SCHEMA}.payments_encrypted\n",
    "\"\"\")\n",
    "\n",
    "display(decrypted_df)\n",
    "\n",
    "print(\"\\nüìù Key Points:\")\n",
    "print(\"   ‚Ä¢ Encryption protects data even if someone gains database access\")\n",
    "print(\"   ‚Ä¢ Decryption requires the encryption key\")\n",
    "print(\"   ‚Ä¢ Key management is critical - use Azure Key Vault, AWS KMS, or GCP KMS\")\n",
    "print(\"   ‚Ä¢ Combine with RBAC to control who can decrypt specific columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Format-Preserving Encryption (FPE)\n",
    "\n",
    "**What is Format-Preserving Encryption?**\n",
    "\n",
    "FPE encrypts data while maintaining its original format. For example, a 16-digit credit card number remains a 16-digit number after encryption, making it compatible with existing systems that expect specific data formats.\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úì Maintains data format constraints (length, character set)\n",
    "- ‚úì Compatible with legacy systems and applications\n",
    "- ‚úì No schema changes required\n",
    "- ‚úì Useful for credit cards, phone numbers, IDs\n",
    "\n",
    "**Implementation Approaches:**\n",
    "\n",
    "1. **FF1/FF3 Algorithms:** NIST-approved FPE algorithms\n",
    "2. **Custom UDFs:** Implement FPE logic via external libraries\n",
    "3. **Third-party Services:** Use specialized FPE vendors\n",
    "\n",
    "**Limitations:**\n",
    "- More complex than standard encryption\n",
    "- May require external libraries or services\n",
    "- Performance overhead compared to standard AES\n",
    "\n",
    "**Use Cases:**\n",
    "- PCI-DSS compliance for credit card masking\n",
    "- Phone number protection while maintaining format\n",
    "- Legacy system integration\n",
    "- Testing with realistic but protected data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format-Preserving Encryption Simulation\n",
    "# NOTE: True FPE requires specialized libraries (e.g., pyffx, ff3)\n",
    "# This demonstrates the concept using a simplified approach\n",
    "\n",
    "print(\"Format-Preserving Encryption (Simulated)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Note: This is a simplified simulation for demonstration purposes.\")\n",
    "print(\"   Production FPE should use NIST-approved FF1/FF3 algorithms.\")\n",
    "print(\"   Consider using libraries like pyffx or services like Protegrity.\\n\")\n",
    "\n",
    "# Create a simplified FPE function for credit card numbers\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION {CATALOG}.{ENCRYPTION_SCHEMA}.fpe_encrypt_cc(cc STRING, key STRING)\n",
    "RETURNS STRING\n",
    "RETURN \n",
    "    -- Simulate FPE by preserving format while changing digits\n",
    "    -- In production, use proper FF1/FF3 algorithms\n",
    "    CONCAT(\n",
    "        SUBSTRING(cc, 1, 4), '-',\n",
    "        LPAD(CAST(MOD(CAST(SUBSTRING(cc, 6, 4) AS INT) + 1234, 10000) AS STRING), 4, '0'), '-',\n",
    "        LPAD(CAST(MOD(CAST(SUBSTRING(cc, 11, 4) AS INT) + 5678, 10000) AS STRING), 4, '0'), '-',\n",
    "        LPAD(CAST(MOD(CAST(SUBSTRING(cc, 16, 4) AS INT) + 9012, 10000) AS STRING), 4, '0')\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Simulated FPE function created\\n\")\n",
    "\n",
    "# Apply FPE to credit card numbers\n",
    "fpe_df = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    customer_name,\n",
    "    credit_card AS original_cc,\n",
    "    {CATALOG}.{ENCRYPTION_SCHEMA}.fpe_encrypt_cc(credit_card, 'key') AS encrypted_cc,\n",
    "    transaction_amount\n",
    "FROM \n",
    "    {CATALOG}.{ENCRYPTION_SCHEMA}.sensitive_data\n",
    "\"\"\")\n",
    "\n",
    "print(\"Original vs Format-Preserving Encrypted Credit Cards:\")\n",
    "display(fpe_df)\n",
    "\n",
    "print(\"\\nüìù Key Observations:\")\n",
    "print(\"   ‚Ä¢ Format is preserved (XXXX-XXXX-XXXX-XXXX)\")\n",
    "print(\"   ‚Ä¢ Length remains the same\")\n",
    "print(\"   ‚Ä¢ Compatible with systems expecting 16-digit cards\")\n",
    "print(\"   ‚Ä¢ In production, use proper FPE libraries for security\")\n",
    "print(\"\\nüîó Production FPE Options:\")\n",
    "print(\"   ‚Ä¢ pyffx library (Python)\")\n",
    "print(\"   ‚Ä¢ Protegrity or Voltage SecureData (Commercial)\")\n",
    "print(\"   ‚Ä¢ AWS Payment Cryptography (AWS-specific)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Envelope Encryption with Unity Catalog\n",
    "\n",
    "**What is Envelope Encryption?**\n",
    "\n",
    "Envelope encryption is a multi-layer encryption approach where data is encrypted with a Data Encryption Key (DEK), and the DEK is then encrypted with a Key Encryption Key (KEK). This provides enhanced security through key separation.\n",
    "\n",
    "**How It Works:**\n",
    "- Step 1: Data is encrypted with a DEK\n",
    "- Step 2: The DEK is encrypted with a KEK\n",
    "- Step 3: Only the encrypted DEK is stored with the data\n",
    "- Step 4: The KEK is managed by a key management service\n",
    "\n",
    "**Benefits:**\n",
    "- Enhanced security through key separation\n",
    "- Efficient key rotation (only re-encrypt DEKs, not data)\n",
    "- Centralized key management\n",
    "- Compliance with regulatory requirements\n",
    "- Protection even if encrypted data is compromised\n",
    "\n",
    "**Unity Catalog Implementation:**\n",
    "\n",
    "Databricks Unity Catalog supports envelope encryption through automatic DEK generation for each table, KEK management via cloud provider KMS, transparent encryption/decryption at runtime, and key rotation without data re-encryption.\n",
    "\n",
    "**Architecture Components:**\n",
    "- Each Delta table has its own DEK\n",
    "- DEKs are encrypted with workspace KEK\n",
    "- KEK stored in cloud KMS (AWS KMS, Azure Key Vault, GCP Cloud KMS)\n",
    "- Transparent access for authorized users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envelope Encryption Demonstration (Conceptual)\n",
    "\n",
    "print(\"Envelope Encryption with Unity Catalog\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìù Note: Envelope encryption is automatically handled by Unity Catalog\")\n",
    "print(\"   when customer-managed keys (CMK) are configured.\\n\")\n",
    "\n",
    "print(\"Encryption Layers:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Layer 1: Data Encryption\")\n",
    "print(\"  ‚Üí Each table has a unique Data Encryption Key (DEK)\")\n",
    "print(\"  ‚Üí Data is encrypted with DEK using AES-256\")\n",
    "print(\"  ‚Üí DEK is generated automatically by Unity Catalog\\n\")\n",
    "\n",
    "print(\"Layer 2: Key Encryption\")\n",
    "print(\"  ‚Üí DEK is encrypted with Key Encryption Key (KEK)\")\n",
    "print(\"  ‚Üí KEK is stored in your cloud provider's KMS\")\n",
    "print(\"  ‚Üí Options: AWS KMS, Azure Key Vault, GCP Cloud KMS\\n\")\n",
    "\n",
    "print(\"Layer 3: Access Control\")\n",
    "print(\"  ‚Üí User requests data from Unity Catalog\")\n",
    "print(\"  ‚Üí Unity Catalog retrieves encrypted DEK\")\n",
    "print(\"  ‚Üí KEK from KMS decrypts the DEK\")\n",
    "print(\"  ‚Üí DEK decrypts the data\")\n",
    "print(\"  ‚Üí Data returned to authorized user\\n\")\n",
    "\n",
    "# Demonstrate the concept with a simplified example\n",
    "print(\"Simplified Example:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Simulate DEK and KEK\n",
    "dek = \"DataKey12345678\"  # In reality, this is random and unique per table\n",
    "kek = \"MasterKey8765432\"  # In reality, this is in your KMS\n",
    "\n",
    "# Create a table (DEK is automatically generated by Unity Catalog)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {CATALOG}.{ENCRYPTION_SCHEMA}.highly_sensitive_data (\n",
    "    id INT,\n",
    "    patient_id STRING,\n",
    "    diagnosis STRING,\n",
    "    treatment STRING\n",
    ")\n",
    "TBLPROPERTIES (\n",
    "    'comment' = 'Protected with envelope encryption'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{ENCRYPTION_SCHEMA}.highly_sensitive_data VALUES\n",
    "    (1, 'P001', 'Type 2 Diabetes', 'Metformin'),\n",
    "    (2, 'P002', 'Hypertension', 'Lisinopril'),\n",
    "    (3, 'P003', 'Asthma', 'Albuterol')\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n‚úì Table created with automatic envelope encryption\")\n",
    "print(\"‚Üí Unity Catalog automatically:\")\n",
    "print(\"  1. Generated a unique DEK for this table\")\n",
    "print(\"  2. Encrypted the table data with the DEK\")\n",
    "print(\"  3. Encrypted the DEK with your workspace KEK\")\n",
    "print(\"  4. Stored the encrypted DEK with table metadata\\n\")\n",
    "\n",
    "display(spark.sql(f\"SELECT * FROM {CATALOG}.{ENCRYPTION_SCHEMA}.highly_sensitive_data\"))\n",
    "\n",
    "print(\"\\nüîí Security Benefits:\")\n",
    "print(\"   ‚Ä¢ Data encrypted at rest with unique keys per table\")\n",
    "print(\"   ‚Ä¢ Keys protected by cloud provider KMS\")\n",
    "print(\"   ‚Ä¢ Key rotation doesn't require re-encrypting all data\")\n",
    "print(\"   ‚Ä¢ Meets compliance requirements (HIPAA, GDPR, etc.)\")\n",
    "print(\"   ‚Ä¢ Audit trail for all key access via KMS logs\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Databricks Multi-key Protection\n",
    "\n",
    "Multi-key protection combines Databricks-managed keys with customer-managed keys to provide enhanced security and control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-key Protection Overview\n",
    "\n",
    "print(\"Databricks Multi-key Protection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nKey Management Architecture:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n1. Databricks-managed Keys (Default)\")\n",
    "print(\"   ‚úì Automatic encryption for all data\")\n",
    "print(\"   ‚úì No configuration required\")\n",
    "print(\"   ‚úì Managed by Databricks\")\n",
    "print(\"   ‚úì Provides baseline security\\n\")\n",
    "\n",
    "print(\"2. Customer-managed Keys (CMK)\")\n",
    "print(\"   ‚úì Keys stored in YOUR cloud KMS\")\n",
    "print(\"   ‚úì YOU control key access and rotation\")\n",
    "print(\"   ‚úì YOU can revoke Databricks access\")\n",
    "print(\"   ‚úì Enhanced compliance and control\\n\")\n",
    "\n",
    "print(\"3. Combined Approach (Recommended)\")\n",
    "print(\"   ‚úì Data encrypted with Databricks DEKs\")\n",
    "print(\"   ‚úì DEKs encrypted with YOUR CMK\")\n",
    "print(\"   ‚úì Best of both worlds\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Configuration Steps (High-level):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nFor AWS:\")\n",
    "print(\"  1. Create CMK in AWS KMS\")\n",
    "print(\"  2. Grant Databricks IAM role access to CMK\")\n",
    "print(\"  3. Configure workspace to use CMK\")\n",
    "print(\"  4. Enable CMK for managed services and storage\")\n",
    "\n",
    "print(\"\\nFor Azure:\")\n",
    "print(\"  1. Create key in Azure Key Vault\")\n",
    "print(\"  2. Configure managed identity\")\n",
    "print(\"  3. Grant Databricks access to Key Vault\")\n",
    "print(\"  4. Enable CMK in workspace settings\")\n",
    "\n",
    "print(\"\\nFor GCP:\")\n",
    "print(\"  1. Create encryption key in Cloud KMS\")\n",
    "print(\"  2. Grant Databricks service account access\")\n",
    "print(\"  3. Configure CMEK in workspace\")\n",
    "print(\"  4. Apply to storage buckets and resources\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Security Benefits:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì Complete control over encryption keys\")\n",
    "print(\"‚úì Ability to revoke access instantly\")\n",
    "print(\"‚úì Separate encryption for workspace, notebooks, and jobs\")\n",
    "print(\"‚úì Compliance with regulations requiring CMK\")\n",
    "print(\"‚úì Audit logs in your cloud KMS\")\n",
    "print(\"‚úì Key rotation without service disruption\")\n",
    "\n",
    "print(\"\\nüìñ Documentation:\")\n",
    "print(\"   AWS: https://docs.databricks.com/security/keys/customer-managed-keys-aws.html\")\n",
    "print(\"   Azure: https://docs.databricks.com/security/keys/customer-managed-keys-azure.html\")\n",
    "print(\"   GCP: https://docs.databricks.com/security/keys/customer-managed-keys-gcp.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "data_encryption",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
