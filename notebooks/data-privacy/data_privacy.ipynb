{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Data Privacy and Security in Databricks\n",
    "\n",
    "This notebook demonstrates data privacy and security features in Databricks Unity Catalog.\n",
    "\n",
    "## Topics:\n",
    "1. **RBAC** - Role-Based Access Control\n",
    "2. **Views** - Dynamic, Restricted, and Materialized\n",
    "3. **Data Hashing** - Irreversible anonymization\n",
    "4. **Data Masking** - Format-preserving obfuscation\n",
    "5. **Row Filtering** - Scope access by attributes\n",
    "6. **Tokenization** - Reversible token replacement\n",
    "7. **ABAC** - Attribute-Based Access Control\n",
    "8. **Encryption** - Protect data at rest and in transit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your demo preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Configuration\n",
    "USE_TEMP_TABLES = True  # Recommended for demos - auto-cleanup on session end\n",
    "\n",
    "# Catalog and schema names\n",
    "CATALOG = \"main\"\n",
    "HR_SCHEMA = \"hr\"\n",
    "CUSTOMERS_SCHEMA = \"customers\"\n",
    "RETAIL_SCHEMA = \"retail\"\n",
    "GOVERNANCE_SCHEMA = \"governance\"\n",
    "\n",
    "print(f\"\u2713 Configuration loaded\")\n",
    "print(f\"  \u2192 Temporary tables: {USE_TEMP_TABLES}\")\n",
    "print(f\"  \u2192 Catalog: {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cell below to set up the demo environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup notebook (creates tables and sample data in background)\n",
    "%run ./setup_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and define helper function\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def get_table_type():\n",
    "    \"\"\"Helper to add TEMPORARY keyword when using temp tables\"\"\"\n",
    "    return \"TEMPORARY\" if USE_TEMP_TABLES else \"\"\n",
    "\n",
    "print(\"\u2713 Libraries loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "masking_header",
   "metadata": {},
   "source": [
    "---\n\n## 4. Data Masking\n\n**What is Data Masking?**\nReplaces sensitive values with obfuscated versions while maintaining format and structure.\n\n**Use Cases:**\n- Display masked SSNs (XXX-XX-6789) instead of raw values\n- Preserve formats for analytics while hiding true values\n- Automate masking by user/group with Unity Catalog\n\n**Key Functions:** `IS_ACCOUNT_GROUP_MEMBER()`, `IS_MEMBER()`, `mask()`\n\n**Important:** Fine-grained controls require serverless compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "masking_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masking function based on group membership\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {CATALOG}.{GOVERNANCE_SCHEMA}.mask_ssn(ssn STRING)\nRETURNS STRING\nRETURN CASE\n    WHEN IS_ACCOUNT_GROUP_MEMBER('admin') THEN ssn\n    ELSE '***-**-****'\nEND\"\"\")\n\n# Create view with masked SSN\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {CATALOG}.{RETAIL_SCHEMA}.v_customers_masked AS\nSELECT id, {CATALOG}.{GOVERNANCE_SCHEMA}.mask_ssn(ssn) AS ssn, name, region\nFROM {CATALOG}.{RETAIL_SCHEMA}.customers\"\"\")\n\nprint(\"Original Data:\")\ndisplay(spark.sql(f\"SELECT * FROM {CATALOG}.{RETAIL_SCHEMA}.customers LIMIT 3\"))\n\nprint(\"\\nMasked Data (SSN hidden based on permissions):\")\ndisplay(spark.sql(f\"SELECT * FROM {CATALOG}.{RETAIL_SCHEMA}.v_customers_masked LIMIT 3\"))\n\nprint(\"\\n\u2713 SSN masked based on group membership\")\nprint(\"\u2713 Admins see full SSN, others see masked\")\nprint(\"\u2713 Format preserved for analytics\")\nprint(\"\\n\u26a0\ufe0f  Update 'admin' to your admin group name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filtering_header",
   "metadata": {},
   "source": [
    "---\n\n## 5. Row-Level Filtering\n\n**What is Row Filtering?**\nControls which records users can view by applying row-level conditions, enforced transparently at query time.\n\n**Use Cases:**\n- GDPR: Restrict EU data to EU employees only\n- Multi-tenancy: Each customer sees only their data\n- Financial segmentation: Business units see only their accounts\n- Data sharing: Curated datasets for external partners\n\n**Benefits:** Transparent enforcement \u2022 Combines with column masks \u2022 No data duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filtering_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create row filter function based on region\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {CATALOG}.{GOVERNANCE_SCHEMA}.filter_by_region(region STRING)\nRETURNS BOOLEAN\nRETURN CASE\n    WHEN IS_MEMBER('Team_US') AND region = 'US' THEN TRUE\n    WHEN IS_MEMBER('Team_EU') AND region = 'EU' THEN TRUE\n    WHEN IS_MEMBER('Team_APAC') AND region = 'APAC' THEN TRUE\n    WHEN IS_ACCOUNT_GROUP_MEMBER('admin') THEN TRUE\n    ELSE FALSE\nEND\"\"\")\n\n# Create view with row filtering\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {CATALOG}.{RETAIL_SCHEMA}.v_customers_filtered AS\nSELECT id, ssn, name, region\nFROM {CATALOG}.{RETAIL_SCHEMA}.customers\nWHERE {CATALOG}.{GOVERNANCE_SCHEMA}.filter_by_region(region)\"\"\")\n\nprint(\"All Data (5 rows):\")\ndisplay(spark.sql(f\"SELECT * FROM {CATALOG}.{RETAIL_SCHEMA}.customers ORDER BY id\"))\n\nprint(\"\\nFiltered Data (based on user's region):\")\ndisplay(spark.sql(f\"SELECT * FROM {CATALOG}.{RETAIL_SCHEMA}.v_customers_filtered ORDER BY id\"))\n\nprint(\"\\n\u2713 Team_US: US records only\")\nprint(\"\u2713 Team_EU: EU records only\")\nprint(\"\u2713 Team_APAC: APAC records only\")\nprint(\"\u2713 Admins: All records\")\nprint(\"\\n\u26a0\ufe0f  Update Team_US, Team_EU, Team_APAC to your group names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization_header",
   "metadata": {},
   "source": [
    "---\n\n## 6. Data Tokenization\n\n**What is Tokenization?**\nSubstitutes sensitive values with random tokens that map back via secure vaults. Unlike hashing, tokenization is **reversible**.\n\n**Use Cases:**\n- PCI-DSS: Replace credit cards with compliant tokens\n- Testing: Provide realistic but protected test data\n- Analytics: Enable analysis without exposing PII\n- Fraud detection: Reversible for authorized investigation\n\n**Production Integration:** VGS, Basis Theory, TokenEx\n\n**Trade-offs:** \u2713 Reversible \u2022 \u26a0\ufe0f Requires external service \u2022 \u26a0\ufe0f Performance overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenization_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Simplified demo - production uses external vault services\n\n# Create tokenization functions\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {CATALOG}.{GOVERNANCE_SCHEMA}.tokenize(value STRING)\nRETURNS STRING\nRETURN CONCAT('TOK-', substr(sha2(value, 256), 1, 32))\"\"\")\n\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} FUNCTION {CATALOG}.{GOVERNANCE_SCHEMA}.detokenize(token STRING, original STRING)\nRETURNS STRING\nRETURN CASE\n    WHEN IS_ACCOUNT_GROUP_MEMBER('admin') THEN original\n    ELSE token\nEND\"\"\")\n\n# Create table with tokenized values\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} TABLE {CATALOG}.{RETAIL_SCHEMA}.customers_tokenized AS\nSELECT id, {CATALOG}.{GOVERNANCE_SCHEMA}.tokenize(ssn) AS ssn_token, ssn AS ssn_original, name, region\nFROM {CATALOG}.{RETAIL_SCHEMA}.customers\"\"\")\n\n# Create view with conditional detokenization\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {CATALOG}.{RETAIL_SCHEMA}.v_customers_tokenized AS\nSELECT id, {CATALOG}.{GOVERNANCE_SCHEMA}.detokenize(ssn_token, ssn_original) AS ssn, name, region\nFROM {CATALOG}.{RETAIL_SCHEMA}.customers_tokenized\"\"\")\n\nprint(\"Tokenized Storage:\")\ndisplay(spark.sql(f\"SELECT id, ssn_token, name, region FROM {CATALOG}.{RETAIL_SCHEMA}.customers_tokenized LIMIT 3\"))\n\nprint(\"\\nConditional Detokenization:\")\ndisplay(spark.sql(f\"SELECT * FROM {CATALOG}.{RETAIL_SCHEMA}.v_customers_tokenized LIMIT 3\"))\n\nprint(\"\\n\u2713 Non-admins see tokens only\")\nprint(\"\u2713 Admins see original values\")\nprint(\"\\n\ud83d\udd17 Production: VGS \u2022 Basis Theory \u2022 TokenEx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac_header",
   "metadata": {},
   "source": [
    "---\n\n## 7. Attribute-Based Access Control (ABAC)\n\n**What is ABAC?**\nPolicy-driven access control based on object attributes (tags). Permissions set and enforced dynamically as data evolves.\n\n**Use Cases:**\n- Auto-deny access to columns tagged 'sensitivity=PII'\n- Monitor and protect credit card data automatically\n- Apply policies to new tables/columns with matching tags\n\n**Key Features:** Tag-based policies \u2022 Dynamic enforcement \u2022 Centralized governance\n\n**Status:** Currently in **Beta** (October 2025)\n\n[ABAC Documentation](https://docs.databricks.com/security/attribute-based-access-control.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABAC Conceptual Workflow (requires Beta workspace configuration)\n\nprint(\"ABAC Workflow (Conceptual)\")\nprint(\"=\"*70)\n\nprint(\"Step 1: Tag column as PII\")\nprint(\"  ALTER TABLE hr.employee_info\")\nprint(\"  ALTER COLUMN ssn SET TAGS ('sensitivity' = 'PII');\\n\")\n\nprint(\"Step 2: Create policy to mask PII\")\nprint(\"  CREATE POLICY mask_pii ON SCHEMA hr\")\nprint(\"  COLUMN MASK (ssn) USING '***-**-****'\")\nprint(\"  TO all_accounts EXCEPT hr_admins;\\n\")\n\nprint(\"Step 3: Apply policy\")\nprint(\"  ALTER TABLE hr.employee_info\")\nprint(\"  ALTER COLUMN ssn SET MASK POLICY mask_pii;\\n\")\n\nprint(\"=\"*70)\n\n# Simulate with view\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {CATALOG}.{HR_SCHEMA}.v_employee_info_abac AS\nSELECT id, name, salary,\n    CASE WHEN IS_ACCOUNT_GROUP_MEMBER('hr_admin') THEN ssn ELSE '***-**-****' END AS ssn\nFROM {CATALOG}.{HR_SCHEMA}.employee_info\"\"\")\n\nprint(\"Simulated ABAC Behavior:\")\ndisplay(spark.sql(f\"SELECT * FROM {CATALOG}.{HR_SCHEMA}.v_employee_info_abac\"))\n\nprint(\"\\n\u2713 Policy auto-masks columns tagged as PII\")\nprint(\"\u2713 Applies to all tables in schema\")\nprint(\"\u2713 Exceptions for specific groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encryption_header",
   "metadata": {},
   "source": [
    "---\n\n## 8. Data Encryption\n\n**What is Encryption?**\nProtects data at rest and in transit by converting to encoded format readable only with decryption keys.\n\n**Databricks Encryption Options:**\n\n1. **AES Functions** - Column-level encryption (`AES_ENCRYPT`, `AES_DECRYPT`)\n2. **Server-side** - Automatic cloud storage encryption (S3, Azure Blob, GCS)\n3. **Format-Preserving** - Encrypt while maintaining format\n4. **Envelope Encryption** - Multi-layer DEK/KEK approach\n5. **Multi-key Protection** - Customer + Databricks managed keys\n\n**See `data_encryption.ipynb` for detailed encryption demonstrations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encryption_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AES-128 Encryption Demo\nencryption_key = \"MySecureKey12345\"  # \u26a0\ufe0f Use Azure Key Vault/AWS KMS/GCP KMS in production\n\n# Create table with encrypted SSN\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} TABLE {CATALOG}.{HR_SCHEMA}.employee_info_encrypted AS\nSELECT id, name, salary, base64(aes_encrypt(ssn, '{encryption_key}', 'ECB', 'PKCS')) AS ssn_encrypted\nFROM {CATALOG}.{HR_SCHEMA}.employee_info\"\"\")\n\nprint(\"Encrypted Data:\")\ndisplay(spark.sql(f\"SELECT * FROM {CATALOG}.{HR_SCHEMA}.employee_info_encrypted\"))\n\n# Create view with conditional decryption\nspark.sql(f\"\"\"CREATE OR REPLACE {get_table_type()} VIEW {CATALOG}.{HR_SCHEMA}.v_employee_info_decrypted AS\nSELECT id, name, salary,\n    CASE\n        WHEN IS_ACCOUNT_GROUP_MEMBER('hr_admin')\n        THEN aes_decrypt(unbase64(ssn_encrypted), '{encryption_key}', 'ECB', 'PKCS')\n        ELSE '***-**-****'\n    END AS ssn\nFROM {CATALOG}.{HR_SCHEMA}.employee_info_encrypted\"\"\")\n\nprint(\"\\nConditionally Decrypted:\")\ndisplay(spark.sql(f\"SELECT * FROM {CATALOG}.{HR_SCHEMA}.v_employee_info_decrypted\"))\n\nprint(\"\\n\u2713 HR admins see decrypted values\")\nprint(\"\u2713 Others see masked values\")\nprint(\"\\n\ud83d\udd12 Best Practices:\")\nprint(\"   \u2022 Use customer-managed keys (CMK)\")\nprint(\"   \u2022 Rotate keys regularly\")\nprint(\"   \u2022 Store keys in vault services (Key Vault, KMS)\")\nprint(\"   \u2022 Enable TLS/SSL for data in transit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n\n## Summary: Privacy Features Comparison\n\n| Feature | Use Case | Reversible | Performance | Complexity |\n|---------|----------|------------|-------------|------------|\n| **RBAC** | Role-based permissions | N/A | Low | Low |\n| **Views** | Controlled exposure | N/A | Low-Med | Low |\n| **Hashing** | Anonymization | No | Low | Low |\n| **Masking** | Format-preserving obfuscation | Optional | Low-Med | Medium |\n| **Row Filtering** | Regional/attribute access | N/A | Medium | Medium |\n| **Tokenization** | Reversible PII protection | Yes | Med-High | High |\n| **ABAC** | Policy-driven control | N/A | Medium | Med-High |\n| **Encryption** | At-rest/transit protection | Yes | Low-Med | Medium |\n\n---\n\n## Key Takeaways\n\n\u2713 **Defense in Depth** - Combine techniques for comprehensive protection\n\u2713 **Unity Catalog** - Centralized governance for all privacy controls\n\u2713 **Serverless Compute** - Required for fine-grained controls\n\u2713 **Audit & Compliance** - All controls logged and auditable\n\u2713 **Performance** - Consider impact when implementing complex policies\n\n---\n\n## Resources\n\n- [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n- [Row & Column Filters](https://docs.databricks.com/security/privacy/row-and-column-filters.html)\n- [ABAC](https://docs.databricks.com/security/attribute-based-access-control.html)\n- [Encryption](https://docs.databricks.com/security/encryption/index.html)\n\n---\n\n## Environment Configuration Notes\n\n**Update these values for your Databricks environment:**\n\n**Group Names:**\n- `admin` \u2192 Your admin group\n- `hr_admin` / `hr_viewer_group` \u2192 Your HR groups\n- `Team_US` / `Team_EU` / `Team_APAC` \u2192 Your regional groups\n\n**Encryption:**\n- Replace hardcoded keys with Azure Key Vault / AWS KMS / GCP KMS references\n\n**Catalogs/Schemas:**\n- Adjust in Configuration cell if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_header",
   "metadata": {},
   "source": [
    "---\n\n## Cleanup (Only for Permanent Tables)\n\nIf you used permanent tables (`USE_TEMP_TABLES = False`), run the cleanup cell below.\n\n**Note:** Temporary tables are automatically cleaned up when your session ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup (only runs if using permanent tables)\nif not USE_TEMP_TABLES:\n    print(\"Cleaning up permanent tables and views...\")\n    \n    # Drop views\n    for view in ['employee_info_public', 'v_employee_info_abac', 'v_employee_info_decrypted']:\n        spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{HR_SCHEMA}.{view}\")\n    \n    for view in ['v_customers_private']:\n        spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{CUSTOMERS_SCHEMA}.{view}\")\n    \n    for view in ['v_customers_masked', 'v_customers_filtered', 'v_customers_tokenized']:\n        spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{RETAIL_SCHEMA}.{view}\")\n    \n    # Drop tables\n    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{HR_SCHEMA}.employee_info\")\n    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{HR_SCHEMA}.employee_info_encrypted\")\n    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{CUSTOMERS_SCHEMA}.customer_info\")\n    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{RETAIL_SCHEMA}.customers\")\n    spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{RETAIL_SCHEMA}.customers_tokenized\")\n    \n    # Drop functions\n    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.mask_ssn\")\n    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.filter_by_region\")\n    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.tokenize\")\n    spark.sql(f\"DROP FUNCTION IF EXISTS {CATALOG}.{GOVERNANCE_SCHEMA}.detokenize\")\n    \n    print(\"\u2713 Cleanup complete\")\nelse:\n    print(\"Using temporary tables - no cleanup needed!\")\n    print(\"Tables will be automatically removed when session ends.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}